# -*- coding: utf-8 -*-
"""lvadsusr72_Aryan_mittal_lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ukNSMH6ZnSNXxlfBpFG-VRCFLQ-Dq0hj
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/booking.csv")

df.head()

df.shape

df.info()

#checking the null values
df.isnull().sum()

df.describe()

plt.figure(figsize=(10, 6))
sns.boxplot(data=df)
plt.title("Boxplot of Numerical Features")
plt.xticks(rotation=45)
plt.show()

#removing the outliers
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

df_no_outliers = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

#boxplot after removing the outlier
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_no_outliers)
plt.title("Boxplot of Numerical Features (Outliers Removed)")
plt.xticks(rotation=45)
plt.show()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

df['room type'] = label_encoder.fit_transform(df['room type'])
df['type of meal'] = label_encoder.fit_transform(df['type of meal'])
df['booking status'] = label_encoder.fit_transform(df['booking status'])

print("Encoded room type categories:", label_encoder.classes_)
print("Encoded meal type categories:", label_encoder.classes_)
print("Encoded booking status categories:", label_encoder.classes_)

df.head()

#feature selection and data cleaning
correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

df = df.drop_duplicates() #removing duplicates

df.corr()        ## geting correlation

df = df.drop(columns=['date of reservation'])   #no need of this columm as dependent feature not effected by it

from sklearn.model_selection import train_test_split
X = df.drop(columns=['booking status','Booking_ID','market segment type'])
y = df['booking status']

#splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

#training the model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize Logistic Regression model
logreg_model = LogisticRegression(max_iter=1000)

# Train the model on the training set
logreg_model.fit(X_train, y_train)

# Predict on the testing set
y_pred = logreg_model.predict(X_test)

y_pred_decoded = label_encoder.inverse_transform(y_pred)
print("Decoded Predictive Values:")
print(y_pred_decoded)

"""**Sigmoid Function (Logistic Function):**
The Sigmoid function, also known as the logistic function, is a mathematical function that maps any real-valued number to a value between 0 and 1.

**Role in Logistic Regression:**
In Logistic Regression, the goal is to predict the probability that a given input belongs to a certain class (e.g., whether a booking is confirmed or not). However, linear regression models can't output probabilities directly because they can produce values outside the range of [0, 1].

By using the Sigmoid function, Logistic Regression converts the raw output of the linear model into a probability score, which can then be interpreted as the likelihood of belonging to a particular class. It is this probabilistic interpretation that makes Logistic Regression suitable for binary classification tasks.
"""

#model evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Calculate precision, recall, and F1-score
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
print("\nConfusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Visualize confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

y_pred_decoded = label_encoder.inverse_transform(y_pred)

