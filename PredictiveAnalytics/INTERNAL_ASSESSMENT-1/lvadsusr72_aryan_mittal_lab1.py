# -*- coding: utf-8 -*-
"""lvadsusr72_aryan_mittal_lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n806_5T_lfEkEaHSo7BYEcF-xtYgGDnt
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#importing the dataset into dataframe
df = pd.read_csv("/content/expenses.csv")

df.head()

df.info()

#checking null values
df.isnull().sum()

#checking outliers
import seaborn as sns
sns.boxplot(df)

#removing the outliers from the dataset
from scipy import stats

z_scores = stats.zscore(df[['age', 'bmi', 'charges']])
abs_z_scores = abs(z_scores)
outliers = (abs_z_scores > 3).any(axis=1)

df = df[~outliers]

#now check using the boxplot and scatterplot

sns.boxplot(df)

sns.scatterplot(df['charges'])

#encoding categorical data
from sklearn.preprocessing import LabelEncoder

cat_features = df.select_dtypes(include=['object'])

enc = LabelEncoder()

for col in cat_features.columns:
  df[col] = enc.fit_transform(df[col])

df.head()

#feature selection and data cleaning
# we need to see the correlation between the features

df.corr()

sns.pairplot(df)

df = df.drop_duplicates() #removing duplicates

#as we can see that only age,bmi,smoker and children are seeing correlated with the charges attibute so we take these as an independent features
X = df[['age','bmi','children','smoker']]
y = df['charges']

#data splitting

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.4, random_state=10)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

X_test

#model development and training
from sklearn.linear_model import LinearRegression

model = LinearRegression()

model.fit(X_train, y_train)

print("coefficient or slope:",model.coef_)
print("Intercept: ",model.intercept_)

y_pred = model.predict(X_test)

y_pred

"""**Importance of learning rate**
Convergence Speed: The learning rate influences how quickly or slowly the optimization algorithm converges to the optimal solution. A larger learning rate leads to faster convergence but increases the risk of overshooting the minimum or oscillating around it. Conversely, a smaller learning rate may converge slowly but provides more stable convergence.

Stability: An appropriately chosen learning rate helps maintain stability during the optimization process. If the learning rate is too large, the optimization algorithm may fail to converge, diverge, or oscillate around the minimum. On the other hand, if it's too small, convergence may be excessively slow, leading to long training times.

Optimal Solution: The learning rate affects the quality of the final solution obtained by the optimization algorithm. A well-tuned learning rate helps the algorithm to find a solution that minimizes the loss function and generalizes well to unseen data.

**Role of Derivatives and Partial Derivatives:**
Gradient Descent: Derivatives and partial derivatives are essential in optimization algorithms like gradient descent, which is used to minimize the loss function during model training. The derivative of the loss function with respect to each parameter (weights and biases) indicates the direction of steepest ascent or descent in the parameter space.

Update Rule: In gradient descent optimization algorithms, such as stochastic gradient descent (SGD) and its variants (e.g., Adam, RMSProp), derivatives (or gradients) of the loss function are used to update the model parameters iteratively. By taking steps in the direction opposite to the gradient, the algorithm aims to reach the minimum of the loss function.
"""

#model evaluation
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

r2 = r2_score(y_test, y_pred)
print("R-squared (R2):", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

